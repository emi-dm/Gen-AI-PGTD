{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa69c25f",
   "metadata": {},
   "source": [
    "# üß† Vector Quantized Variational Autoencoders con MedNIST Dataset\n",
    "\n",
    "üéØ **¬°Hoy ser√°s t√∫ quien entrene un modelo VQVAE!** (Vector Quantized Variational Autoencoder) para aprender representaciones discretas de im√°genes m√©dicas. El VQVAE es un modelo generativo s√∫per poderoso que combina la codificaci√≥n variacional con la cuantizaci√≥n vectorial, lo que permite aprender representaciones discretas de datos continuos. üí™\n",
    "\n",
    "üè• **¬øPor qu√© es importante?** Este enfoque ha demostrado ser incre√≠blemente eficaz en tareas de compresi√≥n y generaci√≥n de im√°genes m√©dicas.\n",
    "\n",
    "üöÄ **¬°Vas a ser capaz de entrenar un modelo VQVAE que puede reconstruir im√°genes m√©dicas como un profesional!** \n",
    "\n",
    "üéì **Mi plan de entrenamiento para ti:** Entrenaremos nuestro modelo VQVAE para que sea capaz de reconstruir las im√°genes de entrada. Trabajaremos con el conjunto de datos MedNIST disponible en MONAI (https://docs.monai.io/en/stable/apps.html#monai.apps.MedNISTDataset). \n",
    "\n",
    "‚ö° **Para entrenar m√°s r√°pido**, he seleccionado solo una de las clases disponibles (\"HeadCT\"), resultando en un conjunto de entrenamiento con 7999 im√°genes 2D.\n",
    "\n",
    "üí° **Dato curioso**: El VQVAE tambi√©n se puede utilizar como un modelo generativo si entrenas un modelo autorregresivo (por ejemplo, PixelCNN, Transformer Decoder) en las representaciones latentes discretas del cuello de botella VQVAE. ¬°Eso est√° fuera del alcance de este tutorial, pero es fascinante! üåü\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167a850",
   "metadata": {},
   "source": [
    "## üîß ¬°Configurando tu entorno de trabajo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46909773",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ae5e8",
   "metadata": {},
   "source": [
    "## üì¶ ¬°Importando nuestras herramientas m√°gicas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d85fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import L1Loss\n",
    "\n",
    "from monai import transforms as mt\n",
    "from monai.apps import MedNISTDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.utils import first, set_determinism, ensure_tuple\n",
    "from monai.networks.nets import VQVAE\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b6e14",
   "metadata": {},
   "source": [
    "## üìÅ Estableciendo tu espacio de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ee071",
   "metadata": {},
   "source": [
    "## üé≤ ¬°Configurando la reproducibilidad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b010865",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049661aa",
   "metadata": {},
   "source": [
    "## üì• ¬°Descargando tus datos m√©dicos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8522d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MedNISTDataset(root_dir=root_dir, section=\"training\", download=True, seed=0)\n",
    "train_datalist = [{\"image\": item[\"image\"]} for item in train_data.data if item[\"class_name\"] == \"HeadCT\"]\n",
    "image_size = 64\n",
    "batch_size = 16\n",
    "\n",
    "train_transforms = mt.Compose(\n",
    "    [\n",
    "        mt.LoadImaged(keys=[\"image\"]),\n",
    "        mt.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        mt.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "        mt.RandAffined(\n",
    "            keys=[\"image\"],\n",
    "            rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],\n",
    "            translate_range=[(-1, 1), (-1, 1)],\n",
    "            scale_range=[(-0.05, 0.05), (-0.05, 0.05)],\n",
    "            spatial_size=[image_size, image_size],\n",
    "            padding_mode=\"zeros\",\n",
    "            prob=0.5,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d437adbd",
   "metadata": {},
   "source": [
    "## üëÄ ¬°Visualizando tus datos de entrenamiento!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3 examples from the training set\n",
    "check_data = first(train_loader)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "for image_n in range(3):\n",
    "    ax[image_n].imshow(check_data[\"image\"][image_n, 0, :, :], cmap=\"gray\")\n",
    "    ax[image_n].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ca19a",
   "metadata": {},
   "source": [
    "## üîç ¬°Preparando tus datos de validaci√≥n!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = MedNISTDataset(root_dir=root_dir, section=\"validation\", download=True, seed=0)\n",
    "val_datalist = [{\"image\": item[\"image\"]} for item in val_data.data if item[\"class_name\"] == \"HeadCT\"]\n",
    "val_transforms = mt.Compose(\n",
    "    [\n",
    "        mt.LoadImaged(keys=[\"image\"]),\n",
    "        mt.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        mt.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ]\n",
    ")\n",
    "val_ds = Dataset(data=val_datalist, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=4, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa9906",
   "metadata": {},
   "source": [
    "## üèóÔ∏è ¬°Construyendo tu red VQVAE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9708f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "model = VQVAE(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=(256, 256),\n",
    "    num_res_channels=256,\n",
    "    num_res_layers=2,\n",
    "    downsample_parameters=((2, 4, 1, 1), (2, 4, 1, 1)),\n",
    "    upsample_parameters=((2, 4, 1, 1, 0), (2, 4, 1, 1, 0)),\n",
    "    num_embeddings=256,\n",
    "    embedding_dim=32,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\n",
    "l1_loss = L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331aa4fc",
   "metadata": {},
   "source": [
    "## üöÄ ¬°Entrenando tu modelo VQVAE!\n",
    "\n",
    "‚è∞ **¬°Prep√°rate para la aventura!** Vamos a entrenar el modelo durante 100 √©pocas. \n",
    "\n",
    "‚ö° **Tiempo estimado**: ~60 minutos (¬°perfecto para tomar un caf√© y ver c√≥mo tu modelo aprende!) ‚òï\n",
    "\n",
    "üí° **Mi consejo**: Mientras entrena, observa c√≥mo evolucionan las m√©tricas de p√©rdida. ¬°Es fascinante ver el aprendizaje en tiempo real!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b06b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "val_interval = 10\n",
    "epoch_recon_loss_list = []\n",
    "epoch_quant_loss_list = []\n",
    "val_recon_epoch_loss_list = []\n",
    "intermediary_images = []\n",
    "n_example_images = 4\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # model outputs reconstruction and the quantization error\n",
    "        reconstruction, quantization_loss = model(images=images)\n",
    "\n",
    "        recons_loss = l1_loss(reconstruction.float(), images.float())\n",
    "\n",
    "        loss = recons_loss + quantization_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += recons_loss.item()\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\"recons_loss\": epoch_loss / (step + 1), \"quantization_loss\": quantization_loss.item() / (step + 1)}\n",
    "        )\n",
    "    epoch_recon_loss_list.append(epoch_loss / (step + 1))\n",
    "    epoch_quant_loss_list.append(quantization_loss.item() / (step + 1))\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_step, batch in enumerate(val_loader, start=1):\n",
    "                images = batch[\"image\"].to(device)\n",
    "\n",
    "                reconstruction, quantization_loss = model(images=images)\n",
    "\n",
    "                # get the first sample from the first validation batch for\n",
    "                # visualizing how the training evolves\n",
    "                if val_step == 1:\n",
    "                    intermediary_images.append(reconstruction[:n_example_images, 0])\n",
    "\n",
    "                recons_loss = l1_loss(reconstruction.float(), images.float())\n",
    "\n",
    "                val_loss += recons_loss.item()\n",
    "\n",
    "        val_loss /= val_step\n",
    "        val_recon_epoch_loss_list.append(val_loss)\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f5e08",
   "metadata": {},
   "source": [
    "## üìà ¬°Analizando tus curvas de aprendizaje!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb14535",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Learning Curves\", fontsize=20)\n",
    "plt.plot(np.linspace(1, max_epochs, max_epochs), epoch_recon_loss_list, color=\"C0\", linewidth=2.0, label=\"Train\")\n",
    "plt.plot(\n",
    "    np.linspace(val_interval, max_epochs, int(max_epochs / val_interval)),\n",
    "    val_recon_epoch_loss_list,\n",
    "    color=\"C1\",\n",
    "    linewidth=2.0,\n",
    "    label=\"Validation\",\n",
    ")\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7b3b4",
   "metadata": {},
   "source": [
    "## üé¨ ¬°Visualizando la evoluci√≥n de tus im√°genes generadas!\n",
    "\n",
    "üåü **¬°Esta es mi parte favorita!** Aqu√≠ podr√°s ver c√≥mo tu modelo mejora progresivamente a lo largo del entrenamiento. ¬°Es como ver una pel√≠cula del aprendizaje de tu IA! üé•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383495dc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot every evaluation as a new line and example as columns\n",
    "val_samples = np.linspace(val_interval, max_epochs, int(max_epochs / val_interval))\n",
    "fig, ax = plt.subplots(nrows=len(val_samples), ncols=1, sharey=True)\n",
    "ax = ensure_tuple(ax)\n",
    "fig.set_size_inches(18.5, 30.5)\n",
    "for image_n in range(len(val_samples)):\n",
    "    reconstructions = torch.reshape(intermediary_images[image_n], (64 * n_example_images, 64)).T\n",
    "    ax[image_n].imshow(reconstructions.cpu(), cmap=\"gray\")\n",
    "    ax[image_n].set_xticks([])\n",
    "    ax[image_n].set_yticks([])\n",
    "    ax[image_n].set_ylabel(f\"Epoch {val_samples[image_n]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f51ea",
   "metadata": {},
   "source": [
    "## üñºÔ∏è ¬°El momento de la verdad: tus im√°genes reconstruidas!\n",
    "\n",
    "üéâ **¬°Lleg√≥ el momento m√°s emocionante!** Vamos a comparar las im√°genes originales con las que tu modelo ha reconstruido. ¬øQu√© tan bien crees que lo ha hecho? ü§î‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a2f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].imshow(images[0, 0].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].title.set_text(\"Inputted Image\")\n",
    "ax[1].imshow(reconstruction[0, 0].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].title.set_text(\"Reconstruction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c56d3",
   "metadata": {},
   "source": [
    "## üßπ ¬°Limpiando tu espacio de trabajo!\n",
    "\n",
    "üóÇÔ∏è **¬°Hora de ser ordenado!** Eliminamos el directorio temporal si fue usado.\n",
    "\n",
    "üí° **Mi recomendaci√≥n**: Siempre es buena pr√°ctica limpiar los archivos temporales al finalizar tus experimentos. ¬°Tu sistema te lo agradecer√°! üôè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "auto:light,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
