{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa69c25f",
   "metadata": {},
   "source": [
    "# 🧠 Vector Quantized Variational Autoencoders con MedNIST Dataset\n",
    "\n",
    "🎯 **¡Hoy serás tú quien entrene un modelo VQVAE!** (Vector Quantized Variational Autoencoder) para aprender representaciones discretas de imágenes médicas. El VQVAE es un modelo generativo súper poderoso que combina la codificación variacional con la cuantización vectorial, lo que permite aprender representaciones discretas de datos continuos. 💪\n",
    "\n",
    "🏥 **¿Por qué es importante?** Este enfoque ha demostrado ser increíblemente eficaz en tareas de compresión y generación de imágenes médicas.\n",
    "\n",
    "🚀 **¡Vas a ser capaz de entrenar un modelo VQVAE que puede reconstruir imágenes médicas como un profesional!** \n",
    "\n",
    "🎓 **Mi plan de entrenamiento para ti:** Entrenaremos nuestro modelo VQVAE para que sea capaz de reconstruir las imágenes de entrada. Trabajaremos con el conjunto de datos MedNIST disponible en MONAI (https://docs.monai.io/en/stable/apps.html#monai.apps.MedNISTDataset). \n",
    "\n",
    "⚡ **Para entrenar más rápido**, he seleccionado solo una de las clases disponibles (\"HeadCT\"), resultando en un conjunto de entrenamiento con 7999 imágenes 2D.\n",
    "\n",
    "💡 **Dato curioso**: El VQVAE también se puede utilizar como un modelo generativo si entrenas un modelo autorregresivo (por ejemplo, PixelCNN, Transformer Decoder) en las representaciones latentes discretas del cuello de botella VQVAE. ¡Eso está fuera del alcance de este tutorial, pero es fascinante! 🌟\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167a850",
   "metadata": {},
   "source": [
    "## 🔧 ¡Configurando tu entorno de trabajo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46909773",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ae5e8",
   "metadata": {},
   "source": [
    "## 📦 ¡Importando nuestras herramientas mágicas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d85fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import L1Loss\n",
    "\n",
    "from monai import transforms as mt\n",
    "from monai.apps import MedNISTDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.utils import first, set_determinism, ensure_tuple\n",
    "from monai.networks.nets import VQVAE\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b6e14",
   "metadata": {},
   "source": [
    "## 📁 Estableciendo tu espacio de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ee071",
   "metadata": {},
   "source": [
    "## 🎲 ¡Configurando la reproducibilidad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b010865",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049661aa",
   "metadata": {},
   "source": [
    "## 📥 ¡Descargando tus datos médicos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8522d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MedNISTDataset(root_dir=root_dir, section=\"training\", download=True, seed=0)\n",
    "train_datalist = [{\"image\": item[\"image\"]} for item in train_data.data if item[\"class_name\"] == \"HeadCT\"]\n",
    "image_size = 64\n",
    "batch_size = 16\n",
    "\n",
    "train_transforms = mt.Compose(\n",
    "    [\n",
    "        mt.LoadImaged(keys=[\"image\"]),\n",
    "        mt.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        mt.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "        mt.RandAffined(\n",
    "            keys=[\"image\"],\n",
    "            rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],\n",
    "            translate_range=[(-1, 1), (-1, 1)],\n",
    "            scale_range=[(-0.05, 0.05), (-0.05, 0.05)],\n",
    "            spatial_size=[image_size, image_size],\n",
    "            padding_mode=\"zeros\",\n",
    "            prob=0.5,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d437adbd",
   "metadata": {},
   "source": [
    "## 👀 ¡Visualizando tus datos de entrenamiento!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3 examples from the training set\n",
    "check_data = first(train_loader)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "for image_n in range(3):\n",
    "    ax[image_n].imshow(check_data[\"image\"][image_n, 0, :, :], cmap=\"gray\")\n",
    "    ax[image_n].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ca19a",
   "metadata": {},
   "source": [
    "## 🔍 ¡Preparando tus datos de validación!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = MedNISTDataset(root_dir=root_dir, section=\"validation\", download=True, seed=0)\n",
    "val_datalist = [{\"image\": item[\"image\"]} for item in val_data.data if item[\"class_name\"] == \"HeadCT\"]\n",
    "val_transforms = mt.Compose(\n",
    "    [\n",
    "        mt.LoadImaged(keys=[\"image\"]),\n",
    "        mt.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        mt.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ]\n",
    ")\n",
    "val_ds = Dataset(data=val_datalist, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=4, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa9906",
   "metadata": {},
   "source": [
    "## 🏗️ ¡Construyendo tu red VQVAE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9708f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "model = VQVAE(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=(256, 256),\n",
    "    num_res_channels=256,\n",
    "    num_res_layers=2,\n",
    "    downsample_parameters=((2, 4, 1, 1), (2, 4, 1, 1)),\n",
    "    upsample_parameters=((2, 4, 1, 1, 0), (2, 4, 1, 1, 0)),\n",
    "    num_embeddings=256,\n",
    "    embedding_dim=32,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\n",
    "l1_loss = L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331aa4fc",
   "metadata": {},
   "source": [
    "## 🚀 ¡Entrenando tu modelo VQVAE!\n",
    "\n",
    "⏰ **¡Prepárate para la aventura!** Vamos a entrenar el modelo durante 100 épocas. \n",
    "\n",
    "⚡ **Tiempo estimado**: ~60 minutos (¡perfecto para tomar un café y ver cómo tu modelo aprende!) ☕\n",
    "\n",
    "💡 **Mi consejo**: Mientras entrena, observa cómo evolucionan las métricas de pérdida. ¡Es fascinante ver el aprendizaje en tiempo real!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b06b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "val_interval = 10\n",
    "epoch_recon_loss_list = []\n",
    "epoch_quant_loss_list = []\n",
    "val_recon_epoch_loss_list = []\n",
    "intermediary_images = []\n",
    "n_example_images = 4\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # model outputs reconstruction and the quantization error\n",
    "        reconstruction, quantization_loss = model(images=images)\n",
    "\n",
    "        recons_loss = l1_loss(reconstruction.float(), images.float())\n",
    "\n",
    "        loss = recons_loss + quantization_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += recons_loss.item()\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\"recons_loss\": epoch_loss / (step + 1), \"quantization_loss\": quantization_loss.item() / (step + 1)}\n",
    "        )\n",
    "    epoch_recon_loss_list.append(epoch_loss / (step + 1))\n",
    "    epoch_quant_loss_list.append(quantization_loss.item() / (step + 1))\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_step, batch in enumerate(val_loader, start=1):\n",
    "                images = batch[\"image\"].to(device)\n",
    "\n",
    "                reconstruction, quantization_loss = model(images=images)\n",
    "\n",
    "                # get the first sample from the first validation batch for\n",
    "                # visualizing how the training evolves\n",
    "                if val_step == 1:\n",
    "                    intermediary_images.append(reconstruction[:n_example_images, 0])\n",
    "\n",
    "                recons_loss = l1_loss(reconstruction.float(), images.float())\n",
    "\n",
    "                val_loss += recons_loss.item()\n",
    "\n",
    "        val_loss /= val_step\n",
    "        val_recon_epoch_loss_list.append(val_loss)\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f5e08",
   "metadata": {},
   "source": [
    "## 📈 ¡Analizando tus curvas de aprendizaje!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb14535",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Learning Curves\", fontsize=20)\n",
    "plt.plot(np.linspace(1, max_epochs, max_epochs), epoch_recon_loss_list, color=\"C0\", linewidth=2.0, label=\"Train\")\n",
    "plt.plot(\n",
    "    np.linspace(val_interval, max_epochs, int(max_epochs / val_interval)),\n",
    "    val_recon_epoch_loss_list,\n",
    "    color=\"C1\",\n",
    "    linewidth=2.0,\n",
    "    label=\"Validation\",\n",
    ")\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7b3b4",
   "metadata": {},
   "source": [
    "## 🎬 ¡Visualizando la evolución de tus imágenes generadas!\n",
    "\n",
    "🌟 **¡Esta es mi parte favorita!** Aquí podrás ver cómo tu modelo mejora progresivamente a lo largo del entrenamiento. ¡Es como ver una película del aprendizaje de tu IA! 🎥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383495dc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot every evaluation as a new line and example as columns\n",
    "val_samples = np.linspace(val_interval, max_epochs, int(max_epochs / val_interval))\n",
    "fig, ax = plt.subplots(nrows=len(val_samples), ncols=1, sharey=True)\n",
    "ax = ensure_tuple(ax)\n",
    "fig.set_size_inches(18.5, 30.5)\n",
    "for image_n in range(len(val_samples)):\n",
    "    reconstructions = torch.reshape(intermediary_images[image_n], (64 * n_example_images, 64)).T\n",
    "    ax[image_n].imshow(reconstructions.cpu(), cmap=\"gray\")\n",
    "    ax[image_n].set_xticks([])\n",
    "    ax[image_n].set_yticks([])\n",
    "    ax[image_n].set_ylabel(f\"Epoch {val_samples[image_n]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f51ea",
   "metadata": {},
   "source": [
    "## 🖼️ ¡El momento de la verdad: tus imágenes reconstruidas!\n",
    "\n",
    "🎉 **¡Llegó el momento más emocionante!** Vamos a comparar las imágenes originales con las que tu modelo ha reconstruido. ¿Qué tan bien crees que lo ha hecho? 🤔✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a2f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].imshow(images[0, 0].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].title.set_text(\"Inputted Image\")\n",
    "ax[1].imshow(reconstruction[0, 0].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].title.set_text(\"Reconstruction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c56d3",
   "metadata": {},
   "source": [
    "## 🧹 ¡Limpiando tu espacio de trabajo!\n",
    "\n",
    "🗂️ **¡Hora de ser ordenado!** Eliminamos el directorio temporal si fue usado.\n",
    "\n",
    "💡 **Mi recomendación**: Siempre es buena práctica limpiar los archivos temporales al finalizar tus experimentos. ¡Tu sistema te lo agradecerá! 🙏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "auto:light,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
