{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando dígitos en PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "PyTorch es una de las bibliotecas de deep learning más utilizadas en el mundo. Desafortunadamente, a pesar de nuestros esfuerzos por simplificar el código lo más posible, algunas partes de esta sesión práctica pueden parecer un poco crípticas para aquellos que son nuevos en PyTorch. Idealmente, los principiantes deberían primero familiarizarse con PyTorch a través de uno o dos tutoriales como\n",
    "\n",
    "* [Deep Learning Con PyTorch: Un Tutorial de 60 Minutos](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* [Entrenando un Clasificador en CIFAR10](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "\n",
    "antes de programar autoencoders. Desafortunadamente, puede que no tengan suficiente tiempo para revisar estos tutoriales durante el tiempo limitado de la sesión práctica.\n",
    "\n",
    "Por lo tanto, sugerimos que comiencen a experimentar con estos dos notebooks de autoencoders, ajusten algunos hiperparámetros, reentrenar las redes, y hagan preguntas si tienen alguna duda.\n",
    "\n",
    "Posteriormente, si quieren saber más sobre los aspectos técnicos de PyTorch, tómense el tiempo para navegar por estos tutoriales de PyTorch. ¡Verán que PyTorch es una herramienta formidable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comencemos por mirar nuestros datos\n",
    "El conjunto de datos MNIST (Modified National Institute of Standards and Technology) es una base de datos de dígitos escritos a mano que se utiliza comúnmente para entrenar sistemas de procesamiento de imágenes. Este conjunto de datos contiene:\n",
    "\n",
    "- 60,000 imágenes de entrenamiento\n",
    "- 10,000 imágenes de prueba\n",
    "- Cada imagen es de 28x28 píxeles en escala de grises\n",
    "- Los valores de los píxeles van de 0 (negro) a 1 (blanco)\n",
    "- Cada imagen está etiquetada con el dígito que representa (0-9)\n",
    "\n",
    "Es considerado el \"Hola Mundo\" del aprendizaje profundo y es ideal para iniciarse en el reconocimiento de patrones y las redes neuronales.\n",
    "\n",
    "Vamos a visualizarlos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from src_vae.visualization.utils import display_data_samples\n",
    "\n",
    "# MNIST consists of 28x28 images, so the size of the data is\n",
    "data_shape = 28, 28\n",
    "data_size = data_shape[0] * data_shape[1]\n",
    "\n",
    "# Download and prepare data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train = MNIST(root=\"data\", download=True, train=True, transform=transform)\n",
    "mnist_test = MNIST(root=\"data\", download=True, train=False, transform=transform)\n",
    "\n",
    "# Check data by displaying random images\n",
    "samples_indices = np.random.randint(len(mnist_train), size=10)\n",
    "mnist_img_list = [mnist_train[sample_idx][0] for sample_idx in samples_indices]\n",
    "display_data_samples(data=mnist_img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are `mnist_train` and `mnist_test`?  Let's look at it.\n",
    "print(mnist_train)\n",
    "print(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the first training image and its class label\n",
    "sample_image = mnist_train[0][0]  # sample_image is a \"PyTorch tensor\"\n",
    "sample_label = mnist_train[0][1]\n",
    "\n",
    "# Convert the Tensor into a numpy array\n",
    "sample_image_np = sample_image.numpy()\n",
    "print(\"Image size = \", sample_image_np.shape)\n",
    "\n",
    "# Call \"squeeze\" to remove the first dimension\n",
    "sample_image_np = sample_image_np.squeeze(0)\n",
    "print(\"Image size = \", sample_image_np.shape)\n",
    "\n",
    "# Plot\n",
    "plt.imshow(sample_image_np)\n",
    "print(\"The image label is \", sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construyamos un autoencoder determinista profundo\n",
    "\n",
    "Aquí construiremos un autoencoder simple con solo capas **densas** (también conocidas como completamente conectadas) y **ReLUs**. En PyTorch, una capa densa se denomina **Linear**.\n",
    "\n",
    "Tanto el codificador como el decodificador tienen **3 capas** y el espacio latente tiene **32 dimensiones**.\n",
    "\n",
    "Dado que los píxeles tienen valores entre 0 y 1, la última función de activación es una **Sigmoid**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "# Let's define the encoder architecture we want,\n",
    "# with some options to configure the input and output size\n",
    "def make_encoder(data_size, latent_space_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(data_size, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, latent_space_size),\n",
    "    )\n",
    "\n",
    "\n",
    "# Same thing for the decoder\n",
    "def make_decoder(data_size, latent_space_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(latent_space_size, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, data_size),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "\n",
    "# Now let's build our networks, with an arbitrary dimensionality of the latent space\n",
    "# and an input and output size depending on the data.\n",
    "latent_space_size = 2\n",
    "encoder = make_encoder(data_size, latent_space_size)\n",
    "decoder = make_decoder(data_size, latent_space_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas:\n",
    "* ¿Qué vamos a generar?\n",
    "* ¿Cuál es el tamaño del espacio latente del autoencoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def autoencoder_forward_pass(encoder, decoder, x):\n",
    "    \"\"\"AE forward pass.\n",
    "\n",
    "    Args:\n",
    "        encoder: neural net that predicts a latent vector\n",
    "        decoder: neural net that projects a point in the latent space back into the image space\n",
    "        x: batch of N MNIST images\n",
    "\n",
    "    Returns:\n",
    "        loss: crossentropy loss\n",
    "        x_hat: batch of N reconstructed images\n",
    "    \"\"\"\n",
    "    in_shape = x.shape  # Save the input shape\n",
    "    encoder_input = torch.flatten(x, start_dim=1)  # Flatten the 2D image to a 1D tensor (for the linear layer)\n",
    "    z = encoder(encoder_input)  # Forward pass on the encoder (to get the latent space vector)\n",
    "    x_hat = decoder(z)  # Forward pass on the decoder (to get the reconstructed input)\n",
    "    x_hat = x_hat.reshape(in_shape)  # Restore the output to the original shape\n",
    "    loss = F.binary_cross_entropy(x_hat, x)  # Compute the reconstruction loss\n",
    "    return loss, x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define some training hyperparameters\n",
    "epochs = 25\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "def train(forward_pass_fn, encoder, decoder, optimizer, train_data, val_data, device=\"cuda\"):\n",
    "    # Create dataloaders from the data\n",
    "    # Those are PyTorch's abstraction to help iterate over the data\n",
    "    data_loader_kwargs = {\"batch_size\": batch_size, \"num_workers\": os.cpu_count() - 1, \"pin_memory\": True}\n",
    "    train_dataloader = DataLoader(train_data, shuffle=True, **data_loader_kwargs)\n",
    "    val_dataloader = DataLoader(val_data, **data_loader_kwargs)\n",
    "\n",
    "    # Ensure that the networks are on the requested device (typically a GPU)\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    fit_pbar = tqdm(range(epochs), desc=\"Training\", unit=\"epoch\")\n",
    "    pbar_metrics = {\"train_loss\": None, \"val_loss\": None}\n",
    "    for epoch in fit_pbar:\n",
    "        # Train once over all the training data\n",
    "        for x, _ in train_dataloader:\n",
    "            x = x.to(device)  # Move the data tensor to the device\n",
    "            optimizer.zero_grad()  # Make sure gradients are reset\n",
    "            train_loss, _ = forward_pass_fn(encoder, decoder, x)  # Forward pass\n",
    "            train_loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update parameters w.r.t. optimizer and gradients\n",
    "            pbar_metrics[\"train_loss\"] = train_loss.item()\n",
    "            fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "        # At the end of the epoch, check performance against the validation data\n",
    "        for x, _ in val_dataloader:\n",
    "            x = x.to(device)  # Move the data tensor to the device\n",
    "            val_loss, _ = forward_pass_fn(encoder, decoder, x)\n",
    "            pbar_metrics[\"val_loss\"] = val_loss.item()\n",
    "            fit_pbar.set_postfix(pbar_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([*encoder.parameters(), *decoder.parameters()])\n",
    "train(autoencoder_forward_pass, encoder, decoder, optimizer, mnist_train, mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a visualizar los resultados para el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from src_vae.visualization.utils import display_autoencoder_results\n",
    "\n",
    "display_autoencoder_results(mnist_test, lambda x: autoencoder_forward_pass(encoder, decoder, x.cuda())[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Espacio latente\n",
    "\n",
    "Antes de pasar al autoencoder variacional, regresa al inicio de este notebook y reemplaza el tamaño del espacio latente de 32 a 2 dimensiones y vuelve a entrenar el autoencoder.\n",
    "\n",
    "Una vez hecho esto, ejecuta la siguiente celda para visualizar el espacio latente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if the autoencoder has a latent space size of 2.\n",
    "\n",
    "from src_vae.visualization.latent_space import explore_latent_space\n",
    "\n",
    "latent_space_size = 2\n",
    "\n",
    "explore_latent_space(\n",
    "    mnist_test,\n",
    "    lambda x: encoder(torch.flatten(x, start_dim=1)),\n",
    "    lambda z: decoder(z).reshape(data_shape),\n",
    "    encodings_label=\"target\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta:\n",
    "\n",
    "¿Por qué crees que con un espacio latente de 2 dimensiones obtenemos imágenes reconstruidas menos precisas (más borrosas)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convirtamos nuestro autoencoder en variacional\n",
    "\n",
    "Los autoencoders variacionales (VAE) son muy similares a los autoencoders. Las diferencias son tres:\n",
    "\n",
    "* El codificador del VAE genera vectores de media y varianza  \n",
    "* La entrada del decodificador es un vector muestreado aleatoriamente de una distribución Normal determinada por los vectores de media y varianza predichos\n",
    "* La función de pérdida tiene 2 términos: la pérdida de reconstrucción (como en el AE normal) + la divergencia KL (para la salida del codificador)\n",
    "\n",
    "Como el gradiente no puede retropropagarse a través de un método de muestreo aleatorio, los VAE siempre vienen con un **truco de reparametrización**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "# Esta vez, empezamos directamente con un espacio latente de 2 dimensiones para visualizarlo fácilmente después\n",
    "latent_space_size = 2\n",
    "\n",
    "# En la práctica, un pequeño truco para implementar fácilmente las dos salidas del \n",
    "# codificador es simplemente duplicar el tamaño de su salida. Luego, podemos dividir \n",
    "# la salida por la mitad durante el paso hacia adelante!\n",
    "vae_encoder = make_encoder(data_size, latent_space_size * 2)\n",
    "vae_decoder = make_decoder(data_size, latent_space_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas:\n",
    "\n",
    "* En la celda anterior, usamos la misma función para construir las redes del codificador y decodificador del VAE que para el AE. La única diferencia es que el tamaño de salida del codificador está multiplicado por 2. ¿Por qué crees que es así?\n",
    "* En la siguiente celda, incluimos el **truco de reparametrización** en el **paso hacia adelante**. ¿Recuerdas por qué esto tiene que hacerse?\n",
    "* ¿Cuál es el tamaño del espacio latente del VAE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necesitamos cambiar el algoritmo de entrenamiento para implementar el \"truco de reparametrización\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "def kl_div(mu, logvar):\n",
    "    kl_div_by_samples = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    return torch.mean(kl_div_by_samples)\n",
    "\n",
    "\n",
    "def vae_forward_pass(encoder, decoder, x):\n",
    "    \"\"\"VAE forward pass.\n",
    "\n",
    "    Args:\n",
    "        encoder: neural net that predicts a mean and a logvar vector\n",
    "        decoder: neural net that projects a point in the latent space back into the image space\n",
    "        x: batch of N MNIST images\n",
    "\n",
    "    Returns:\n",
    "        loss: crossentropy + kl_divergence loss\n",
    "        x_hat: batch of N reconstructed images\n",
    "    \"\"\"\n",
    "    in_shape = x.shape  # Save the input shape\n",
    "    encoder_input = torch.flatten(x, start_dim=1)  # Flatten the 2D image to a 1D tensor (for the linear layer)\n",
    "    encoding_distr = encoder(encoder_input)  # Forward pass on the encoder (to get the latent space posterior)\n",
    "    # Nothing changed so far!\n",
    "\n",
    "    # Second part of our trick!\n",
    "    # We separate the (unique) latent space posterior into its two halves: mean and logvar\n",
    "    mu, logvar = encoding_distr[:, :latent_space_size], encoding_distr[:, latent_space_size:]\n",
    "\n",
    "    # Reparametrization trick\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    z = mu + eps * std\n",
    "\n",
    "    # Decoding mostly stays the same. The only difference is the added 4th line below\n",
    "    x_hat = decoder(z)  # Forward pass on the decoder (to get the reconstructed input)\n",
    "    x_hat = x_hat.reshape(in_shape)  # Restore the output to the original shape\n",
    "    loss = F.binary_cross_entropy(x_hat, x)  # Compute the reconstruction loss\n",
    "    loss += 5e-3 * kl_div(mu, logvar)  # Loss now also includes the KL divergence term\n",
    "    return loss, x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Vamos a entrenar nuestro VAE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([*vae_encoder.parameters(), *vae_decoder.parameters()])\n",
    "train(vae_forward_pass, vae_encoder, vae_decoder, optimizer, mnist_train, mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo a los resultados de nuestro VAE entrenado. Recuerda que el VAE tiene un espacio latente de 2 dimensiones, así que vamos a visualizarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "display_autoencoder_results(mnist_test, lambda x: vae_forward_pass(vae_encoder, vae_decoder, x.cuda())[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Más visualizaciones\n",
    "\n",
    "Ahora que tenemos un espacio latente en dos dimensiones, podemos visualizarlo fácilmente y observar cómo se distribuyen los datos.\n",
    "\n",
    "### ¿Ves la diferencia entre este espacio latente y el del autoencoder anterior?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from src_vae.visualization.latent_space import explore_latent_space\n",
    "\n",
    "explore_latent_space(\n",
    "    mnist_test,\n",
    "    lambda x: vae_encoder(torch.flatten(x, start_dim=1))[:, :latent_space_size],\n",
    "    lambda z: vae_decoder(z).reshape(data_shape),\n",
    "    encodings_label=\"target\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En la siguiente celda, decodificaremos un vector `z` seleccionado del espacio latente. ¡Cambia el contenido de ese vector y verás qué sucede! \n",
    "\n",
    "Prueba con [-56,5], ¿qué sucede?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = [-1, -1]  # 2D latent vector\n",
    "\n",
    "z_torch = torch.tensor(z, dtype=torch.float).cuda()  # convert Z into a PyTorch tensor\n",
    "\n",
    "sample = vae_decoder(z_torch).reshape(data_shape)  # decode the latent vector with the VAE decoder\n",
    "\n",
    "plt.imshow(sample.detach().cpu().numpy())  # plot the resulting image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
